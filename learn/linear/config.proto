//  @file   config.proto
//  @brief  configuration for linear method

package dmlc.linear;

message Config {

  /////////////////////////////////////////////////
  //       basic settings - input & output
  /////////////////////////////////////////////////

  // Both train_data and val_data can be either a directory or a wildcard
  // filename such as "part-[0-9].*". also support hdfs:// and s3://
  optional string train_data = 1;
  optional string val_data = 2;

  // data format. support libsvm, crb, criteo, adfea, ...
  optional string data_format = 4 [default = "libsvm"];

  // model output
  optional string model_out = 5;

  // save model for every *save_model* data pass. if 0, then only save the last
  // one
  optional int32 save_model = 6 [default = 0];

  // model input
  optional string model_in = 7;

  // load model from iter = *load_model*. if 0, then load the last one
  optional int32 load_model = 8 [default = 0];


  // output of the prediction results. if specified, then do
  // prediction. otherwise run the training job.
  optional string pred_out = 9;

  ////////////////////////////////////////////////////////////
  //       basic settings - objective and optimization
  ///////////////////////////////////////////////////////////

  // the loss function
  enum Loss {
    SQUARE = 1;
    LOGIT = 2;
    SQUARE_HINGE = 4;
    // HINGE = 3;
  }
  optional Loss loss = 11 [default = LOGIT];

  // elastic-net regularizer:
  //   lambda_l1 * ||w||_1 + .5 * lambda_l2 * ||w||_F^2
  optional float lambda_l1 = 12 [default = 1];
  optional float lambda_l2 = 13 [default = 0];

  // the learning  method
  enum Algo {
    // minibatch SGD
    SGD = 1;     // standard sgd
    ADAGRAD = 2; // adaptive gradient descent
    FTRL = 3;
  }
  optional Algo algo = 21 [default = FTRL];

  // the size of minibatch for minibatch SGD. a small value improves the
  // convergence rate, but may decrease the system performance
  optional int32 minibatch = 22 [default = 1000];

  // the maximal number of data passes
  optional int32 max_data_pass = 23 [default = 10];

  // learning rate, often in the format lr_eta / (lr_beta + x), where x depends
  // on the updater, such as sqrt(iter), or the cumulative gradient on adagrad
  optional float lr_eta = 24 [default = .01];

  //////////////////////////////////////
  //      advanced settings
  //////////////////////////////////////

  // -- data --

  // each worker reads the data in local if the data have been
  // dispatched into workers' local disks. it can reduce the cost to access data
  // remotely
  optional bool use_worker_local_data = 101;

  // virtually partition a file into nparts for better loadbalance.
  optional int32 num_parts_per_file = 102 [default = 10];

  // randomly shuffle data for minibatch SGD. a minibatch is randomly pick from
  // rand_shuffle * minibatch_size examples.
  optional int32 rand_shuffle = 103 [default = 10];

  // down sampling negative examples in the training data
  optional float neg_sampling = 104 [default = 1.0];

  // -- learning --

  // print the progress every n sec
  optional float disp_itv = 111 [default = 1];

  // learning rate, see lr_eta
  optional float lr_beta = 112 [default = 1];


  // -- system performance --

  // number of threads used within a worker and a server
  optional int32 num_threads = 121 [default = 2];

  // maximal allowed delay during synchronization. it is the maximal number of
  // parallel minibatches for SGD, and parallel block for block CD.
  optional int32 max_delay = 122 [default = 0];

  // cache the key list on both sender and receiver to reduce communication
  // cost. it may increase the memory usage
  optional bool key_cache = 123 [default = true];

  // compression the message to reduce communication cost. it may increase the
  // computation cost.
  optional bool msg_compression = 124 [default = true];

  // convert floating-points into fixed-point integers with n bytes. n can be 1,
  // 2 and 3. 0 means no compression.
  optional int32 fixed_bytes = 125 [default = 0];
}
