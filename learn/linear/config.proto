//  @file   config.proto
//  @brief  configuration for linear method

package dmlc.linear;

message Config {
  //
  // data
  //
  optional string train_data = 1;
  optional string val_data = 2;
  optional string data_format = 4 [default = "libsvm"];
  // partition a file into npart
  optional int32 num_parts_per_file = 5 [default = 1];
  optional bool use_worker_local_data = 3;

  //
  // model
  //
  optional string model_in = 10;
  optional string model_out = 11;

  // objective function
  enum Loss {
    SQUARE = 1;
    LOGIT = 2;
    // HINGE = 3;
    SQUARE_HINGE = 4;
  }
  optional Loss loss = 20 [default = LOGIT];

  // regularizer: lambda_l1 * ||w||_1 + .5 * lambda_l2 * ||w||_F^2
  optional float lambda_l1 = 21;
  optional float lambda_l2 = 22;

  //
  // optimization methods
  //
  enum Algo {
    // (minibatch) online methods
    SGD = 1;     // standard sgd
    ADAGRAD = 2; // adaptive gradient descent
    FTRL = 3;
  }
  optional Algo algo = 30 [default = FTRL];

  // the size of minibatch for online methods
  optional int32 minibatch = 31 [default = 1000];

  // the maximal number of data passes
  optional int32 max_data_pass = 32 [default = 10];

  // print the progress every n sec
  optional float disp_itv = 33 [default = 1];

  // learning rate, often in the format lr_eta / (lr_beta + x), where x is
  // user-defined, such as sqrt(iter), or the cumulative gradient on adagrad
  optional float lr_eta = 41;
  optional float lr_beta = 42;

  optional int32 num_threads = 60 [default = 2];
  //
  // to reduce sync cost
  //
  // maximal allowed delay during synchronization
  optional int32 max_delay = 50 [default = 0];
  // communication filters
  optional bool key_cache = 51 [default = true];
  optional bool msg_compression = 52 [default = true];
  // 0 means no fixing
  optional int32 fixed_bytes = 53 [default = 1];


  optional int32 rand_shuffle = 101;
}
