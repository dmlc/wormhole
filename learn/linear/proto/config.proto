///  @file   config.proto
///  @brief  configuration for linear method

package dmlc.linear;

message Config {
  ///
  /// data
  ///
  optional string train_data = 1;
  optional string val_data = 2;
  optional string test_data = 3;
  optional string data_format = 4 [default = "libsvm"];

  // partition a file into npart
  optional int32 num_parts_per_file = 5 [default = 1];
  // limit the number of files send to a worker
  // optional int32 num_files_per_worker = 6 [default = 0];

  ///
  /// model
  ///
  optional string model_in = 10;
  optional string model_out = 11;

  /// objective function
  enum Loss {
    SQUARE = 1;
    LOGIT = 2;
    HINGE = 3;
    SQUARE_HINGE = 4;
  }
  optional Loss loss = 20 [default = LOGIT];

  enum Penalty {
    L1 = 1; // lambda(0) * ||w||_1 + .5 * lambda(1) * ||w||_F^2
    L2 = 2;  // .5 * lambda(0) * ||w||_F^2
  }
  optional Penalty penalty = 21 [default = L1];
  repeated float lambda = 22;

  ///
  /// optimization methods
  ///
  enum Algo {
    // (minibatch) online methods
    SGD = 1;  // standard sgd
    ADAGRAD = 2;  // adaptive gradient descent
    FTRL = 3;

    // batch methdos
    LBFGS = 4;

    // stochastic (block) coordinate descent
    DARLIN = 5;
  }
  optional Algo algo = 30 [default = FTRL];

  // the size of minibatch for online methods
  optional int32 minibatch = 31 [default = 1000];

  // the maximal number of data passes
  optional int32 max_data_pass = 32 [default = 10];

  // print the progress every *show_prog* sec
  optional float disp_itv = 33 [default = 1];

  // convergance critiria. stop if the relative objective <= epsilon
  optional double epsilon = 34 [default = 1e-4];

  // learning rate
  enum LearningRate {
    CONSTANT = 1; // = lr_eta
    DECAY = 2;    // = lr_eta / (lr_beta + x), where x is user-defined, such as
                  // sqrt(iter), or the cumulative gradient on adagrad
  }
  optional LearningRate lr = 40;
  optional float lr_eta = 41;
  optional float lr_beta = 42;

  ///
  /// to reduce sync cost
  ///

  // maximal allowed delay during synchronization
  optional int32 max_delay = 50 [default = 0];

  // features which occurs <= *tail_feature_freq* will be filtered. it save both
  // memory and bandwidth.
  optional int32 tail_feature_freq = 51 [default = 0];

  // filters to reduce communication costs
  // repeated ps.FilterConfig push_filter = 42;
  // repeated ps.FilterConfig pull_filter = 43;

  //
  // method specific options
  //
  // optional Darlin darlin = 100;

  /// debug options
  optional bool test_io = 80 [default = false];
}

// message Darlin {
//   // Divide a feature group into feature_block_ratio x nnz_feature_per_example
//   // blocks
//   optional float feature_block_ratio = 1 [default = 4];
//   // Use a random order to updating feature block.  Turn it off to elimiate the
//   // randomness (often for debuging), but may affects the convergence rate.
//   optional bool random_feature_block_order = 2 [default = true];
//   // Updating important feature group at the beginning to get a good initial
//   // start point.
//   repeated int32 prior_fea_group = 14;
//   optional int32 num_iter_for_prior_fea_group = 13 [default = 5];
//   optional int32 max_num_parallel_groups_in_preprocessing = 9 [default = 1000];
//   // Used by the trust region method. All changes of parameters will be bounded
//   // by *delta*. *delta* is updated according to the convergence,  whose intial
//   // value is *delta_init_value* and maximal value is *delta_max_value*
//   optional double delta_init_value = 101 [default = 1];
//   optional double delta_max_value = 102 [default = 5];
//   // kkt_filter_threshold = max_gradient_violation / num_examples *
//   // kkt_filter_threshold_ratio. increasing this number reduces the effect of
//   // kkt filter.
//   optional double kkt_filter_threshold_ratio = 103 [default = 10];
// }
